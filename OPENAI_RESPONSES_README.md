# OpenAI Responses API Integration

This MCP Platform now supports **OpenAI's Responses API** for reasoning models like `o3`, `o1-preview`, and `o4-mini`. The Responses API is specifically designed for advanced reasoning models that can think step-by-step through complex problems.

## âœ… What's Working

- **âœ… Full Integration**: Responses API integrated into the MCP Platform
- **âœ… Streaming Support**: Real-time token streaming from reasoning models
- **âœ… Model Support**: o3, o1-preview, o4-mini, and other reasoning models
- **âœ… Configuration**: Easy model switching via `config.yaml`
- **âœ… Error Handling**: Robust error handling and retry logic
- **âœ… Production Ready**: HTTP resilience and proper async handling

## ğŸš§ Current Limitations

- **ğŸš§ No MCP Tools**: Tools are temporarily disabled (requires HTTP MCP servers)
- **ğŸš§ No Function Calling**: Reasoning models work best without tools anyway
- **ğŸš§ No Temperature Control**: Reasoning models don't support temperature

## ğŸš€ Quick Start

### 1. Set Your API Key
```bash
# Add to .env file
OPENAI_API_KEY=your_openai_api_key_here
```

### 2. Configure Model
In `src/config.yaml`, set:
```yaml
llm:
  active: "openai_responses"  # Enable Responses API

  providers:
    openai_responses:
      model: "o3"  # or o1-preview, o4-mini, etc.
      max_tokens: 2048
```

### 3. Run the Platform
```bash
uv run src/main.py
```

### 4. Connect via WebSocket
The platform runs on `ws://localhost:8000/ws/chat` by default.

## ğŸ¯ Best Use Cases

The Responses API excels at:

- **ğŸ§  Complex Reasoning**: Multi-step logical problems
- **ğŸ”¬ Analysis Tasks**: Data analysis, research synthesis  
- **ğŸ“ Writing**: Long-form content with structured thinking
- **ğŸ“ Education**: Detailed explanations and tutoring
- **ğŸ” Problem Solving**: Breaking down complex problems

## ğŸ“‹ Supported Models

| Model | Description | Best For |
|-------|-------------|----------|
| `o3` | Latest reasoning model | Complex analysis, research |
| `o1-preview` | Advanced reasoning | Multi-step problems |
| `o4-mini` | Efficient reasoning | Faster responses |

## ğŸ”§ Configuration Options

### Model Settings
```yaml
openai_responses:
  model: "o3"
  max_tokens: 2048    # Max output tokens
  # Note: temperature not supported by reasoning models
```

### WebSocket Settings
```yaml
chat:
  websocket:
    host: "localhost"
    port: 8000
    endpoint: "/ws/chat"
```

## ğŸ”„ HTTP MCP Integration (Future)

Currently, MCP tools are disabled because the Responses API requires **HTTP MCP servers**, while our platform uses **stdio MCP servers**.

### Why HTTP Required?

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     HTTP     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ OpenAI API  â”‚ â†â”€â”€â”€â”€â”€â”€â”€â”€â†’  â”‚ MCP Server  â”‚
â”‚ (remote)    â”‚             â”‚ (HTTP)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

The OpenAI API runs on remote servers and needs to connect to your MCP server over HTTP.

### ğŸ› ï¸ Future HTTP MCP Integration

To enable MCP tools with the Responses API, you'll need:

#### Option 1: Dual Transport Servers
```python
# Run MCP server in both modes
if __name__ == "__main__":
    if "--http" in sys.argv:
        mcp.run(transport="streamable-http")  # For OpenAI
    else:
        mcp.run()  # stdio for our platform
```

#### Option 2: HTTP-Only Architecture
```python
# Convert platform to use HTTP MCP servers
mcp_client = Client("http://localhost:8000/mcp/")
```

#### Option 3: FastMCP Proxy
```python
# Bridge stdio to HTTP
proxy = FastMCP.as_proxy(
    ProxyClient("demo_server.py"),
    name="HTTP Bridge"
)
proxy.run(transport="http", port=8001)
```

### Example HTTP MCP Server Setup

1. **Create HTTP version of your MCP server:**
```python
from mcp.server.fastmcp import FastMCP

mcp = FastMCP("Demo")

@mcp.tool()
def calculate(expression: str) -> float:
    """Evaluate a mathematical expression"""
    return eval(expression)  # Use safely in production!

if __name__ == "__main__":
    mcp.run(transport="streamable-http", port=8001)
```

2. **Update orchestrator to use HTTP MCP:**
```python
# In openai_responses_orchestrator.py
if tools:
    mcp_tool = {
        "type": "mcp",
        "server_url": "http://localhost:8001/mcp/",
        "server_label": "demo", 
        "allowed_tools": [t["name"] for t in tools],
        "require_approval": "never"
    }
    payload["tools"] = [mcp_tool]
```

3. **Start both servers:**
```bash
# Terminal 1: HTTP MCP Server
python Servers/demo_server.py --http

# Terminal 2: Main Platform  
uv run src/main.py
```

## ğŸ” Debugging

### Enable Debug Logging
The orchestrator includes debug logging to inspect payloads:

```python
logger.info(f"DEBUG: Sending payload: {json.dumps(payload, indent=2)}")
```

### Common Issues

1. **400 Bad Request**: Check for unsupported parameters (temperature, tools)
2. **Port Conflicts**: Ensure HTTP MCP server and WebSocket server use different ports
3. **API Key**: Verify `OPENAI_API_KEY` is set correctly

## ğŸ“š Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    WebSocket    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Client    â”‚ â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ â”‚   Platform   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                        â”‚
                                   HTTP POST
                                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   OpenAI    â”‚ â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚  Responses   â”‚
â”‚ Responses   â”‚    (future)     â”‚ Orchestrator â”‚
â”‚     API     â”‚ â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ â”‚              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    MCP Tools    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                        â”‚
                                    stdio
                                        â–¼
                                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                â”‚ MCP Servers  â”‚
                                â”‚   (stdio)    â”‚
                                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“ˆ Performance Notes

- **Reasoning Models**: Generate more tokens than regular models
- **Streaming**: Essential for user experience with long responses  
- **Token Usage**: Monitor usage as reasoning models can be token-heavy
- **Caching**: Consider implementing response caching for repeated queries

## ğŸ‰ Success Story

The integration successfully supports:
- âœ… Real-time streaming responses
- âœ… Complex reasoning queries (quantum physics explanations, etc.)
- âœ… Robust error handling and retries
- âœ… Clean WebSocket interface
- âœ… Production-ready architecture

The platform is now ready for advanced reasoning tasks with OpenAI's most capable models!
