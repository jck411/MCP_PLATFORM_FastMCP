# MCP Chatbot Configuration

# WebSocket Chat Configuration
chat:
  # WebSocket server configuration
  websocket:
    host: "localhost"
    port: 8001  # Changed to avoid port conflict
    endpoint: "/ws/chat"

    # CORS settings
    allow_origins: ["*"]
    allow_credentials: true

    # WebSocket settings
    max_message_size: 16777216  # 16MB
    ping_interval: 20
    ping_timeout: 10

  # Chat Service Configuration
  service:
    # System prompt configuration
    system_prompt: |
      You are a helpful assistant with a sense of humor.
      You have access to to a list of tools, use them when needed and explain why you used them.

    # Streaming configuration
    streaming:
      enabled: true  # Enable streaming by default (required setting)

    # Tool execution notifications
    tool_notifications:
      enabled: true
      show_args: true
      icon: "ðŸ”§"
      format: "{icon} Executing tool: {tool_name}"
      # Available placeholders: {icon}, {tool_name}, {tool_args}

    # Logging configuration for chat service
    logging:
      tool_execution: true
      tool_results: true
      result_truncate_length: 200
      system_prompt: true  # Log the generated system prompt during initialization
      llm_replies: true  # Log ALL LLM replies including internal ones not sent to user
      llm_reply_truncate_length: 500  # Truncate length for LLM reply logs

# LLM Configuration - Just change the 'active' provider!
llm:
  active: "openai_responses"  # Change this to: openai, groq, openrouter, anthropic, azure, gemini, openai_responses

  # HTTP client configuration
  http:
    timeout: 30.0                    # Request timeout in seconds
    max_retries: 3                   # Maximum number of retry attempts
    initial_retry_delay: 1.0         # Initial delay for exponential backoff (seconds)
    max_retry_delay: 16.0            # Maximum delay for exponential backoff (seconds)
    retry_multiplier: 2.0            # Multiplier for exponential backoff
    retry_jitter: 0.1                # Jitter factor to prevent thundering herd
    max_keepalive_connections: 20    # Maximum keep-alive connections
    max_connections: 100             # Maximum total connections
    keepalive_expiry: 30.0           # Keep-alive connection expiry (seconds)

  # Provider presets
  providers:
    openai:
      base_url: "https://api.openai.com/v1"
      model: "gpt-4o-mini"  # or gpt-4, gpt-4-turbo, etc.
      temperature: 0.7
      max_tokens: 4096
      top_p: 1.0

    openai_responses:
      base_url: "https://api.openai.com/v1"
      model: "o3"  # or o1-preview, o3-mini, o4-mini, etc.
      temperature: 0.1
      max_tokens: 2048
      top_p: 1.0

    groq:
      base_url: "https://api.groq.com/openai/v1"
      model: "llama-3.3-70b-versatile"  # or llama-3.1-8b-instant, etc.
      temperature: 0.7
      max_tokens: 4096
      top_p: 1.0

    openrouter:
      base_url: "https://openrouter.ai/api/v1"
      model: "moonshotai/kimi-k2"  # or other available models
      temperature: 0.7
      max_tokens: 4096
      top_p: 1.0

    anthropic:
      base_url: "https://api.anthropic.com/v1"
      model: "claude-sonnet-4-20250514"  # Updated to current model
      temperature: 0.7
      max_tokens: 4096
      top_p: 1.0

    azure:
      base_url: "https://your-resource.openai.azure.com/openai/deployments/your-deployment"
      model: "gpt-4"  # matches your Azure deployment
      temperature: 0.7
      max_tokens: 4096
      top_p: 1.0
      api_version: "2024-02-15-preview"

    gemini:
      base_url: "https://generativelanguage.googleapis.com"
      model: "gemini-1.5-pro"  # or gemini-1.5-flash, gemini-1.0-pro
      temperature: 0.7
      max_tokens: 4096
      top_p: 1.0
      top_k: 40

# Logging Configuration
logging:
  level: "INFO"
  format: "%(asctime)s - %(levelname)s - %(message)s"

# MCP Server Configuration
mcp:
  config_file: "servers_config.json"
